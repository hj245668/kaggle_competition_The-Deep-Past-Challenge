20260104 번역 내용이 아직 좀 이상함 

epoch 수가 적고
기본 mT5-small이 거의 그대로
우선 한 번 제출해서 점수 확인 후 

epoch 늘리기 / lr 조절
published_texts 등 추가 데이터 활용
후처리 조금 더 정제

현재 나의 조건 : 
GPU P100, 시간/세션 제한 있음
현재 mT5-small + 기본 train.csv만 거의 쓰는 수준

목표 설정 :
1단계: BLEU 15~20 (제대로 번역처럼 보이는 수준)
2단계: BLEU 25~30 (중상위권 진입, 꽤 좋은 번역)
3단계: 30 후반 이상은 mT5-base 이상 + 추가 데이터 + 꽤 빡센 튜닝 필요.

1. 지금 상태에서 “당장” 할 수 있는 튜닝들

(1) 모델 바꾸기: mT5-small → mT5-base
small은 파라미터가 적어서 학습은 빠르지만 용량이 부족해서 복잡한 MT에선 한계가 있다.
P100 16GB 기준으로:
google/mt5-base도
batch_size=2
gradient_accumulation_steps=4 정도면

충분히 학습 가능.

실전 팁:
small에서 잘 돌아가는 세팅을 base로 그대로 옮기되,
batch_size만 줄이고 epoch도 살짝 줄여서 OOM 안 나는지 체크.
같은 데이터로 small vs base 비교하면 BLEU 몇 점은 바로 튀어오르는 경우가 많다.

체감상:
small → base로만 바꿔도
“헛소리 → 적어도 번역처럼 보임” 수준 업그레이드가 꽤 자주 나온다.

(2) train/val split 다시 설계
지금은 그냥 길이 기반 층화 정도만 했는데, 조금 더 신경 쓸 수 있다.
문서 단위로 비슷한 텍스트가 train/val에 같이 들어가면
→ val 점수는 높은데 test로는 일반화 잘 안 되는 경우 많음.
할 수 있는 것:
oare_id 기준으로 document-level split 시도
길이 bin + genre 비슷한 것 섞어서 고르게 나누기
완벽하게 할 필요는 없고,
“val을 너무 쉬운 데이터로만 구성하지 않는다”는 정도만 챙겨줘도 일반화에 도움 된다.

(3) 학습 스케줄 튜닝
지금은 대략:
lr 2e-4, epoch 4, gradient_accumulation_steps=2
정도인데, 이걸 살짝 바꿔볼 수 있다.

추천 조합:
워밍업 + 코사인 스케줄러
warmup_ratio=0.1, lr_scheduler_type="cosine"
초반 급격한 폭주 줄이고 후반 미세 조정에 좋음.
epoch 두 단계로 나누기
1차: lr 3e-4, epoch 2–3 (조금 공격적으로)
2차: 저장된 체크포인트에서 lr 1e-4, epoch 2–3 (fine-tuning)
gradient_checkpointing 켜기 (메모리 절약)
model.gradient_checkpointing_enable()
메모리 조금 더 아껴서 batch_size를 약간 키울 수 있음 (effective batch↑)

(4) generate 설정 튜닝

현재는:
num_beams=5
max_length=256, min_length=20
length_penalty=1.1, no_repeat_ngram_size=3

이걸 조금 흔들면서 test BLEU·val BLEU를 비교할 수 있다.

유용한 방향:
num_beams=4~8 사이에서 조정
length_penalty 0.8 ~ 1.2 사이에서 sweep
min_length 5, 10, 20 시험해보기

실전에서는:
너무 길면 BLEU 떨어지고,
너무 짧으면 정보 부족으로 다시 떨어진다.
간단하게는:

val set에 대해 여러 generate 설정으로 번역 →
sacrebleu로 점수 계산해서 제일 높은 세팅을 test에 적용.

2. 큰 점프를 만드는 “데이터 쪽” 튜닝

여기서부터는 진짜 점수 점프를 노리는 부분.
(1) Published Texts + Publications에서 추가 평행데이터 만들기
대회 설명에서 강조하던 부분이 이거였지.
published_texts.csv
transliteration 있음, translation 없는 경우 많음
metadata 통해 다른 리소스랑 매칭 가능
publications.csv
OCR된 논문 텍스트, 여러 언어로 된 번역 포함
할 수 있는 것(난이도 중급 이상):
동일 문서/줄 번호 기준으로 Akkadian–modern language 쌍 찾기
modern language가 English가 아니면:
우선 최신 LLM(예: ChatGPT)으로 English로 통일 (오프라인에서 미리 만들기)
이렇게 모은 데이터를 train에 추가 (mixed fine-tuning)

이 작업이 잘 되면:
train 데이터가 몇 배로 늘어나서
BLEU가 5–10점 이상 점프할 여지가 있다.
다만 이건 꽤 공수 들어가서
“당장 오늘 밤 안에” 보다는 “프로젝트 느낌”에 가깝다.

(2) Lexicon / dictionary feature 활용

OA_Lexicon_eBL.csv, eBL_Dictionary.csv 같은 사전 파일을 활용해서:
입력 Akkadian에 나오는 단어들을
norm/lexeme 기준으로 변환
사전에서 영어 gloss를 찾아 input에 붙여주는 방식

예:
원래 입력:
translate Akkadian to English: KIŠIB ma-nu-ba-lúm-a-šur ...

추가 정보 붙인 입력:
translate Akkadian to English: KIŠIB(maštu) ma-nu-ba-lúm-a-šur(Mannum-balum-Aššur) ...
처럼 “힌트”를 미리 주는 구조.

이건:
모델이 단어 의미를 추론하기 훨씬 쉬워지고
특히 out-of-domain 문장에서도 안정성이 올라간다.

(3) Multi-task / pre-finetune 전략

조금 더 욕심 내면:
일반적인 multi-lingual MT (예: OPUS, Flores 같은 데이터)로
mT5를 먼저 1–2 epoch 가볍게 돌린 뒤
그 다음에 이 Akkadian 데이터로 다시 fine-tuning
이런 식으로 계단식 학습도 있다.

다만 P100 한 장 + 시간제한 환경에선
이건 “있으면 좋지만 필수까진 아님” 쪽.

3. 현실적인 목표와 예상 점수
상황정리 : 
mT5-small + train.csv만 사용 + 지금 수준 셋업 개선

제대로 학습만 굴려도:
BLEU 10–15 정도는 노려볼 수 있음.

mT5-base로 교체 + train.csv + published_texts 일부 병합 + generate 튜닝
공들여 세팅하면:
BLEU 20–25 정도 충분히 가능.

추가 평행데이터 (publications 정제), lexicon 활용까지 하면

시간/노동 많이 들이긴 하지만
BLEU 25–30+ 도 현실적인 범위.

그 이상(30 후반~40대)은
더 큰 모델(mT5-large 등)
더 정교한 데이터 정제
앙상블/다단계 시스템

이 조합이라서,
지금 리소스(P100 한 장) 기준에선 “챌린지” 영역이라고 보면 된다.

4. 우리 다음 스텝 제안

지금 당장 할 만한 “다음 한 스텝”을 뽑자면:

mT5-base로 갈아타기
batch_size=2, grad_accum=4 전후로 세팅해서 OOM 안 나는지 확인
epoch 4–5 정도 돌리고
val set에 대해 generate 설정 몇 개 바꿔가며 BLEU 측정

제일 좋은 세팅으로 test generate → Kaggle 제출

이 루프만 제대로 한번 돌려도
지금 <extra_id_…> 잔뜩 섞인 결과보다는 훨씬 “번역 같은 번역”이 나올 거야.

원하면:

mT5-base용 완전한 새 노트북 템플릿 (셋업 + 학습 + 예측)

또는 “지금 노트북을 base로 바꾸는 패치 버전”

둘 중 하나를 코드로 딱 정리해서 쭉 붙여 줄 수도 있다.
어느 쪽이 좋을지 말해주면 거기에 맞춰서 바로짜줄게.

개발자 모드
