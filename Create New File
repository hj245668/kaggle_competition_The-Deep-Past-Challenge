{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14377185,"sourceType":"datasetVersion","datasetId":9181584}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Past Challenge – P100 Optimized mT5-small Pipeline\n\nGPU : Tesla P100 16GB\n\n- 번역: Akkadian → English\n- 모델: google/mt5-small\n- Epoch: 2 (안정) \n\n1. 최소 라이브러리 설치\n2. 데이터 로드 및 전처리\n3. Train / Validation 분리\n4. mT5-small 학습\n5. 테스트 번역 생성\n6. 제출 파일 저장","metadata":{}},{"cell_type":"code","source":"import torch\n\nprint(\"device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    print(\"name:\", torch.cuda.get_device_name(0))\n    print(\"capability:\", torch.cuda.get_device_capability(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:39:22.935640Z","iopub.execute_input":"2026-01-04T11:39:22.936000Z","iopub.status.idle":"2026-01-04T11:39:26.446063Z","shell.execute_reply.started":"2026-01-04T11:39:22.935951Z","shell.execute_reply":"2026-01-04T11:39:26.445455Z"}},"outputs":[{"name":"stdout","text":"device: cuda\nname: Tesla P100-PCIE-16GB\ncapability: (6, 0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 1. 라이브러리 설치 (최소, 충돌 방지)\n# 평가용 BLEU/chrF를 위해 sacrebleu, evaluate만 최소 설치\n!pip install -q sacrebleu evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:39:31.498194Z","iopub.execute_input":"2026-01-04T11:39:31.498627Z","iopub.status.idle":"2026-01-04T11:39:36.224136Z","shell.execute_reply.started":"2026-01-04T11:39:31.498603Z","shell.execute_reply":"2026-01-04T11:39:36.223436Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\n# Kaggle Secrets에서 키 불러오기\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n# wandb 로그인\nwandb.login(key=wandb_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:39:38.792537Z","iopub.execute_input":"2026-01-04T11:39:38.793191Z","iopub.status.idle":"2026-01-04T11:39:49.333794Z","shell.execute_reply.started":"2026-01-04T11:39:38.793159Z","shell.execute_reply":"2026-01-04T11:39:49.333194Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n  | |_| | '_ \\/ _` / _` |  _/ -_)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoflywith\u001b[0m (\u001b[33msoflywith-university-of-suwon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed\n)\n\nimport evaluate\n\nSEED = 42\nset_seed(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\nif device == \"cuda\":\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n    print(\"Capability:\", torch.cuda.get_device_capability(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:48:07.198643Z","iopub.execute_input":"2026-01-04T11:48:07.199666Z","iopub.status.idle":"2026-01-04T11:48:33.815757Z","shell.execute_reply.started":"2026-01-04T11:48:07.199638Z","shell.execute_reply":"2026-01-04T11:48:33.815103Z"}},"outputs":[{"name":"stderr","text":"2026-01-04 11:48:17.602167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767527297.792877      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767527297.843698      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767527298.302273      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767527298.302320      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767527298.302323      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767527298.302326      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nGPU: Tesla P100-PCIE-16GB\nCapability: (6, 0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# # 4. 전처리 함수 정의\n\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\nsample_sub = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n\nprint(train_df.shape, test_df.shape)\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:57:46.400313Z","iopub.execute_input":"2026-01-04T11:57:46.401526Z","iopub.status.idle":"2026-01-04T11:57:46.492415Z","shell.execute_reply.started":"2026-01-04T11:57:46.401494Z","shell.execute_reply":"2026-01-04T11:57:46.491803Z"}},"outputs":[{"name":"stdout","text":"(1561, 3) (4, 5)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                oare_id  \\\n0  004a7dbd-57ce-46f8-9691-409be61c676e   \n1  0064939c-59b9-4448-a63d-34612af0a1b5   \n2  0073f2c0-524c-4bbf-915a-8c1772a4fb98   \n3  009fb838-8038-42bc-ad34-5f795b3840ee   \n4  00aa1c55-c80c-4346-a159-73ad43ab0ff7   \n\n                                     transliteration  \\\n0  KIŠIB ma-nu-ba-lúm-a-šur DUMU ṣí-lá-(d)IM KIŠI...   \n1               1 TÚG ša qá-tim i-tur₄-DINGIR il₅-qé   \n2  TÚG u-la i-dí-na-ku-um i-tù-ra-ma 9 GÍN KÙ.BABBAR   \n3  KIŠIB šu-(d)EN.LÍL DUMU šu-ku-bi-im KIŠIB ṣí-l...   \n4  um-ma šu-ku-tum-ma a-na IŠTAR-lá-ma-sí ù ni-ta...   \n\n                                         translation  \n0  Seal of Mannum-balum-Aššur son of Ṣilli-Adad, ...  \n1  Itūr-ilī has received one textile of ordinary ...  \n2  ... he did not give you a textile. He returned...  \n3  Seal of Šu-Illil son of Šu-Kūbum, seal of Ṣilū...  \n4  From Šukkutum to Ištar-lamassī and Nitahšušar:...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>oare_id</th>\n      <th>transliteration</th>\n      <th>translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>004a7dbd-57ce-46f8-9691-409be61c676e</td>\n      <td>KIŠIB ma-nu-ba-lúm-a-šur DUMU ṣí-lá-(d)IM KIŠI...</td>\n      <td>Seal of Mannum-balum-Aššur son of Ṣilli-Adad, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0064939c-59b9-4448-a63d-34612af0a1b5</td>\n      <td>1 TÚG ša qá-tim i-tur₄-DINGIR il₅-qé</td>\n      <td>Itūr-ilī has received one textile of ordinary ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0073f2c0-524c-4bbf-915a-8c1772a4fb98</td>\n      <td>TÚG u-la i-dí-na-ku-um i-tù-ra-ma 9 GÍN KÙ.BABBAR</td>\n      <td>... he did not give you a textile. He returned...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>009fb838-8038-42bc-ad34-5f795b3840ee</td>\n      <td>KIŠIB šu-(d)EN.LÍL DUMU šu-ku-bi-im KIŠIB ṣí-l...</td>\n      <td>Seal of Šu-Illil son of Šu-Kūbum, seal of Ṣilū...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00aa1c55-c80c-4346-a159-73ad43ab0ff7</td>\n      <td>um-ma šu-ku-tum-ma a-na IŠTAR-lá-ma-sí ù ni-ta...</td>\n      <td>From Šukkutum to Ištar-lamassī and Nitahšušar:...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"#5. 전처리 함수 정의\n\ndef normalize_akkadian(text):\n    if pd.isna(text):\n        return \"\"\n    text = str(text)\n    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)\n    text = re.sub(r'<([^>]*)>', r'\\1', text)\n    text = re.sub(r'[…⸢⸣]', '', text)\n    text = re.sub(r'\\?+', '', text)\n    text = re.sub(r'-+', '-', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef normalize_translation(text):\n    if pd.isna(text):\n        return \"\"\n    text = str(text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    if len(text) > 0:\n        text = text[0].upper() + text[1:]\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:57:52.785180Z","iopub.execute_input":"2026-01-04T11:57:52.785862Z","iopub.status.idle":"2026-01-04T11:57:52.791582Z","shell.execute_reply.started":"2026-01-04T11:57:52.785833Z","shell.execute_reply":"2026-01-04T11:57:52.790740Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 6. 전처리 적용 및 필터링\n\ntrain_df[\"transliteration\"] = train_df[\"transliteration\"].apply(normalize_akkadian)\ntrain_df[\"translation\"] = train_df[\"translation\"].apply(normalize_translation)\ntest_df[\"transliteration\"] = test_df[\"transliteration\"].apply(normalize_akkadian)\n\n# 노이즈 제거\ntrain_df = train_df[\n    (train_df[\"transliteration\"].str.len() > 3) &\n    (train_df[\"translation\"].str.len() > 3)\n].reset_index(drop=True)\n\nprint(train_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:57:57.722725Z","iopub.execute_input":"2026-01-04T11:57:57.723495Z","iopub.status.idle":"2026-01-04T11:57:57.890350Z","shell.execute_reply.started":"2026-01-04T11:57:57.723465Z","shell.execute_reply":"2026-01-04T11:57:57.889546Z"}},"outputs":[{"name":"stdout","text":"(1561, 3)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 7. Train / Validation 분리\n\ntrain_split, val_split = train_test_split(\n    train_df,\n    test_size=0.05,\n    random_state=SEED\n)\n\ntrain_dataset = Dataset.from_pandas(train_split.reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val_split.reset_index(drop=True))\n\nraw_datasets = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset\n})\n\nraw_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:58:06.975713Z","iopub.execute_input":"2026-01-04T11:58:06.976452Z","iopub.status.idle":"2026-01-04T11:58:07.012753Z","shell.execute_reply.started":"2026-01-04T11:58:06.976403Z","shell.execute_reply":"2026-01-04T11:58:07.011972Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['oare_id', 'transliteration', 'translation'],\n        num_rows: 1482\n    })\n    validation: Dataset({\n        features: ['oare_id', 'transliteration', 'translation'],\n        num_rows: 79\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# 8. mT5-small 로드\n\nmodel_name = \"google/mt5-small\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n\nmax_source_length = 256\nmax_target_length = 256\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:58:10.887444Z","iopub.execute_input":"2026-01-04T11:58:10.887738Z","iopub.status.idle":"2026-01-04T11:58:25.281916Z","shell.execute_reply.started":"2026-01-04T11:58:10.887714Z","shell.execute_reply":"2026-01-04T11:58:25.280691Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"826cd7fa5672498aabe570bdb6a660ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583a3a195e344f18b24c7f61df4f2567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f6cc219f2d4cb4a1243bfce5b1a638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"465e1a3aa57b462489fc4d6c0b1c718a"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9dcb652529f4942a7179f4921ecfb28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a313a5f00e54f6b8eb25d7a24b60e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757241141cf64119b1ba5e76e30b9922"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# 9. data tokenization\n\ndef preprocess_function(examples):\n    inputs = [\"translate Akkadian to English: \" + x for x in examples[\"transliteration\"]]\n    targets = examples[\"translation\"]\n\n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_source_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n\n    labels = tokenizer(\n        text_target=targets,\n        max_length=max_target_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_datasets = raw_datasets.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:59:44.713880Z","iopub.execute_input":"2026-01-04T11:59:44.714202Z","iopub.status.idle":"2026-01-04T11:59:45.709122Z","shell.execute_reply.started":"2026-01-04T11:59:44.714175Z","shell.execute_reply":"2026-01-04T11:59:45.708369Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1482 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"254c41bc1b1c441b849755dbf2ebc7ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/79 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b863bee2b39f4e3a9e3adaa686e5ab31"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:59:51.118283Z","iopub.execute_input":"2026-01-04T11:59:51.118900Z","iopub.status.idle":"2026-01-04T11:59:51.122501Z","shell.execute_reply.started":"2026-01-04T11:59:51.118872Z","shell.execute_reply":"2026-01-04T11:59:51.121715Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# 10. P100 최적 학습 설정 (버전에 맞게 정리)\n\nbatch_size = 4  # P100 안전 값\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./mt5-small-deeppast-p100\",\n\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n\n    gradient_accumulation_steps=2,   # effective batch ~= 8\n\n    num_train_epochs=4,              # 2 → 4 epoch\n    learning_rate=2e-4,              # 3e-4 → 2e-4\n\n    fp16=torch.cuda.is_available(),  # P100에서는 True\n\n    predict_with_generate=True,      # 나중에 쓸 수도 있으니 유지\n    logging_steps=20,                # 이건 예전 버전에서도 지원됨\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:59:54.758061Z","iopub.execute_input":"2026-01-04T11:59:54.758363Z","iopub.status.idle":"2026-01-04T11:59:54.788258Z","shell.execute_reply.started":"2026-01-04T11:59:54.758338Z","shell.execute_reply":"2026-01-04T11:59:54.787587Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrain_result = trainer.train()\nprint(train_result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:59:57.627118Z","iopub.execute_input":"2026-01-04T11:59:57.627848Z","iopub.status.idle":"2026-01-04T12:06:20.398013Z","shell.execute_reply.started":"2026-01-04T11:59:57.627811Z","shell.execute_reply":"2026-01-04T12:06:20.397148Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20260104_115958-yk4bz8fy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/soflywith-university-of-suwon/huggingface/runs/yk4bz8fy' target=\"_blank\">cool-tree-8</a></strong> to <a href='https://wandb.ai/soflywith-university-of-suwon/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/soflywith-university-of-suwon/huggingface' target=\"_blank\">https://wandb.ai/soflywith-university-of-suwon/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/soflywith-university-of-suwon/huggingface/runs/yk4bz8fy' target=\"_blank\">https://wandb.ai/soflywith-university-of-suwon/huggingface/runs/yk4bz8fy</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='744' max='744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [744/744 06:13, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"TrainOutput(global_step=744, training_loss=0.0, metrics={'train_runtime': 382.0626, 'train_samples_per_second': 15.516, 'train_steps_per_second': 1.947, 'total_flos': 1567214083768320.0, 'train_loss': 0.0, 'epoch': 4.0})\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 학습 직후 모델을 따로 저장\n\ntrainer.save_model(\"./mt5-small-deeppast-p100-final\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:28:01.951083Z","iopub.execute_input":"2026-01-04T12:28:01.951361Z","iopub.status.idle":"2026-01-04T12:28:04.649012Z","shell.execute_reply.started":"2026-01-04T12:28:01.951335Z","shell.execute_reply":"2026-01-04T12:28:04.648207Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# 12. 테스트셋 번역 생성 (padding 추가 버전)\n\nfrom datasets import Dataset\n\ntest_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n\ndef preprocess_test(examples):\n    inputs = [\"translate Akkadian to English: \" + x for x in examples[\"transliteration\"]]\n    return tokenizer(\n        inputs,\n        truncation=True,\n        max_length=max_source_length,\n        padding=\"max_length\",  # 이거 추가된 버전\n    )\n\ntokenized_test = test_dataset.map(\n    preprocess_test,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:28:07.069891Z","iopub.execute_input":"2026-01-04T12:28:07.070264Z","iopub.status.idle":"2026-01-04T12:28:07.130768Z","shell.execute_reply.started":"2026-01-04T12:28:07.070233Z","shell.execute_reply":"2026-01-04T12:28:07.130002Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73287039efe349ef832f5f40b02d5028"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# 13번 셀 실행준에 항상 저장된 모델을 다시 로드해서 씀\n\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"./mt5-small-deeppast-p100-final\").to(device)\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:28:10.950169Z","iopub.execute_input":"2026-01-04T12:28:10.950489Z","iopub.status.idle":"2026-01-04T12:28:11.406453Z","shell.execute_reply.started":"2026-01-04T12:28:10.950465Z","shell.execute_reply":"2026-01-04T12:28:11.405705Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"MT5ForConditionalGeneration(\n  (shared): Embedding(250112, 512)\n  (encoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# 13. 예측 수행 (길이 제약 + 빔서치 튜닝)\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nmodel.eval()\nmodel.to(device)\n\ntokenized_test_pt = tokenized_test.with_format(\"torch\")\n\ntest_loader = DataLoader(\n    tokenized_test_pt,\n    batch_size=4,\n)\n\ndecoded = []\n\nfor batch in test_loader:\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=max_target_length,\n            min_length=20,\n            num_beams=5,\n            length_penalty=1.1,\n            no_repeat_ngram_size=3,\n        )\n\n    batch_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    decoded.extend([x.strip() for x in batch_texts])\n\nprint(\"예측 개수:\", len(decoded))\nfor i in range(min(3, len(decoded))):\n    print(f\"[{i}] {decoded[i][:200]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:28:16.810347Z","iopub.execute_input":"2026-01-04T12:28:16.811087Z","iopub.status.idle":"2026-01-04T12:28:17.532432Z","shell.execute_reply.started":"2026-01-04T12:28:16.811054Z","shell.execute_reply":"2026-01-04T12:28:17.531608Z"}},"outputs":[{"name":"stdout","text":"예측 개수: 4\n[0] <extra_id_0> to English to English translations - English to Español - Spanish - Afrikaans\n[1] <extra_id_0> a-nim(ki) <extra_id_10> - English translation of Akkadian to English\n[2] <extra_id_0> translations of Akkadian to English: English translations - English language translations\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# 14. 후처리 +제출 셀\nimport re\n\ndef postprocess_prediction(text: str) -> str:\n    # extra_id 토큰 제거\n    text = re.sub(r\"<extra_id_\\d+>\", \"\", text)\n    \n    bad_phrases = [\n        \"to English translations\",\n        \"English translations\",\n        \"translations of Akkadian to English\",\n        \"English language translations\",\n    ]\n    for bp in bad_phrases:\n        text = text.replace(bp, \"\")\n    \n    text = text.strip(\" -:,. \\n\\t\")\n    if text and text[0].islower():\n        text = text[0].upper() + text[1:]\n    if text and text[-1] not in \".!?\":\n        text += \".\"\n    return text\n\n\nfinal_preds = [postprocess_prediction(t) for t in decoded]\n\nsubmission = sample_sub.copy()\nsubmission[\"translation\"] = final_preds[: len(submission)]\n\nout_name = \"submission.csv\"  \nsubmission.to_csv(out_name, index=False)\n\nprint(\"Saved submission.csv\")\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:28:27.240982Z","iopub.execute_input":"2026-01-04T12:28:27.241672Z","iopub.status.idle":"2026-01-04T12:28:27.261593Z","shell.execute_reply.started":"2026-01-04T12:28:27.241643Z","shell.execute_reply":"2026-01-04T12:28:27.260836Z"}},"outputs":[{"name":"stdout","text":"Saved submission.csv\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   id                                        translation\n0   0  To English  - English to Español - Spanish - A...\n1   1  A-nim(ki)  - English translation of Akkadian t...\n2   2                                                   \n3   3                          Ta-nim ú-mí-ni ú–mí-mi ú.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>To English  - English to Español - Spanish - A...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>A-nim(ki)  - English translation of Akkadian t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Ta-nim ú-mí-ni ú–mí-mi ú.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23}]}